{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "#from pennylane import numpy as np\n",
    "import torch\n",
    "\n",
    "head = [\"A\", \"B\", \"X\", \"Y\", \"C\"]\n",
    "relations = [[\"A\",\"B\",\"X\",\"Y\", \"C\"], [\"X\",\"Y\"], [\"A\",\"X\"], [\"A\",\"Y\"], [\"B\",\"X\"], [\"B\", \"Y\"], [\"C\"]]\n",
    "fds = [[[\"A\", \"B\"], [\"X\", \"Y\", \"C\"]], \n",
    "           [[\"B\", \"X\", \"Y\"], [\"A\", \"C\"]], \n",
    "           [[\"X\", \"C\"], [\"A\", \"B\", \"Y\"]], \n",
    "           [[\"A\", \"X\", \"Y\"], [\"B\", \"C\"]], \n",
    "           [[\"A\", \"C\"], [\"B\", \"X\", \"Y\"]], \n",
    "           [[\"Y\", \"C\"], [\"A\", \"B\", \"X\"]]]\n",
    "rmax = 1\n",
    "\n",
    "n_qubits = len(head)*rmax\n",
    "print(n_qubits)\n",
    "\n",
    "variables_to_marginals = {}\n",
    "for i in range(len(head)):\n",
    "    variables_to_marginals[head[i]] = [j for j in range(i * rmax, (i+1) * rmax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities):\n",
    "    probabilities = torch.clamp(probabilities, 1e-10, 1.0)\n",
    "    entropy = -torch.sum(probabilities * torch.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def circuit(rotations, measured_qubits):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(rotations[i][0], wires=i)\n",
    "        qml.RZ(rotations[i][1], wires=i)\n",
    "        qml.CNOT(wires=[i, (i+1) % n_qubits])\n",
    "    return qml.probs(wires=measured_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_less_one(x):\n",
    "    return torch.where(x > 1, torch.square(x - 1), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_head = [marg for var in head for marg in variables_to_marginals[var]]\n",
    "indices_relations = [[marg for var in relation for marg in variables_to_marginals[var]] for relation in relations]\n",
    "indices1 = [[marg for var in fd[0] + fd[1] for marg in variables_to_marginals[var]] for fd in fds]\n",
    "indices2 = [[marg for var in fd[1] for marg in variables_to_marginals[var]] for fd in fds]\n",
    "\n",
    "def cost_function(weights):\n",
    "    entropy_sum = torch.tensor(0.0)\n",
    "    hu0 = circuit(weights, indices_head)\n",
    "    entropy_sum += -entropy(hu0)\n",
    "    \n",
    "    entropies = torch.stack([entropy(circuit(weights, indices)) for indices in indices_relations])\n",
    "    entropy_sum += torch.sum(cost_less_one(entropies))\n",
    "    \n",
    "    entropies1 = torch.stack([entropy(circuit(weights, indices)) for indices in indices1])\n",
    "    entropies2 = torch.stack([entropy(circuit(weights, indices)) for indices in indices2])\n",
    "    entropy_sum += torch.sum(entropies1 - entropies2)\n",
    "    \n",
    "    return entropy_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loss: 3.836354970932007\n",
      "Step 2: Loss: 1.8560353517532349\n",
      "Step 3: Loss: 0.7422341704368591\n",
      "Step 4: Loss: 0.3020022213459015\n",
      "Step 5: Loss: 0.1459791213274002\n",
      "Step 6: Loss: 0.0629500076174736\n",
      "Step 7: Loss: -0.004909770097583532\n",
      "Step 8: Loss: -0.06699071079492569\n",
      "Step 9: Loss: -0.12144710123538971\n",
      "Step 10: Loss: -0.1551903337240219\n",
      "Step 11: Loss: -0.18495559692382812\n",
      "Step 12: Loss: -0.20584332942962646\n",
      "Step 13: Loss: -0.21061336994171143\n",
      "Step 14: Loss: -0.216363325715065\n",
      "Step 15: Loss: -0.22685085237026215\n",
      "Step 16: Loss: -0.24367322027683258\n",
      "Step 17: Loss: -0.26818186044692993\n",
      "Step 18: Loss: -0.3013626039028168\n",
      "Step 19: Loss: -0.34332647919654846\n",
      "Step 20: Loss: -0.39275839924812317\n",
      "Step 21: Loss: -0.44637376070022583\n",
      "Step 22: Loss: -0.49783581495285034\n",
      "Step 23: Loss: -0.5468140244483948\n",
      "Step 24: Loss: -0.5973817110061646\n",
      "Step 25: Loss: -0.6488492488861084\n",
      "Step 26: Loss: -0.7030434608459473\n",
      "Step 27: Loss: -0.760107159614563\n",
      "Step 28: Loss: -0.8168932795524597\n",
      "Step 29: Loss: -0.8686746954917908\n",
      "Step 30: Loss: -0.9098853468894958\n",
      "Step 31: Loss: -0.9386811256408691\n",
      "Step 32: Loss: -0.9560158252716064\n",
      "Step 33: Loss: -0.9677481055259705\n",
      "Step 34: Loss: -0.9729164242744446\n",
      "Step 35: Loss: -0.9696853756904602\n",
      "Step 36: Loss: -0.9639074802398682\n",
      "Step 37: Loss: -0.9563585519790649\n",
      "Step 38: Loss: -0.950208842754364\n",
      "Step 39: Loss: -0.9485222101211548\n",
      "Step 40: Loss: -0.95139080286026\n",
      "Step 41: Loss: -0.9571252465248108\n",
      "Step 42: Loss: -0.9634328484535217\n",
      "Step 43: Loss: -0.9681163430213928\n",
      "Step 44: Loss: -0.9705386161804199\n",
      "Step 45: Loss: -0.9705535173416138\n",
      "Step 46: Loss: -0.9705906510353088\n",
      "Step 47: Loss: -0.9716808199882507\n",
      "Step 48: Loss: -0.9726534485816956\n",
      "Step 49: Loss: -0.9727645516395569\n",
      "Step 50: Loss: -0.9724076986312866\n",
      "Step 51: Loss: -0.9729690551757812\n",
      "Step 52: Loss: -0.9754368662834167\n",
      "Step 53: Loss: -0.9797365069389343\n",
      "Step 54: Loss: -0.9849961996078491\n",
      "Step 55: Loss: -0.9899125695228577\n",
      "Step 56: Loss: -0.9934765100479126\n",
      "Step 57: Loss: -0.9954676032066345\n",
      "Step 58: Loss: -0.9965779781341553\n",
      "Step 59: Loss: -0.9969821572303772\n",
      "Step 60: Loss: -0.9966269731521606\n",
      "Step 61: Loss: -0.9958639144897461\n",
      "Step 62: Loss: -0.9951727390289307\n",
      "Step 63: Loss: -0.9951470494270325\n",
      "Step 64: Loss: -0.9958019852638245\n",
      "Step 65: Loss: -0.9967291355133057\n",
      "Step 66: Loss: -0.9973601698875427\n",
      "Step 67: Loss: -0.997385561466217\n",
      "Step 68: Loss: -0.9970449805259705\n",
      "Step 69: Loss: -0.9966448545455933\n",
      "Step 70: Loss: -0.9963001012802124\n",
      "Step 71: Loss: -0.9960885643959045\n",
      "Step 72: Loss: -0.9959743618965149\n",
      "Step 73: Loss: -0.9961528778076172\n",
      "Step 74: Loss: -0.9966492056846619\n",
      "Step 75: Loss: -0.9973548650741577\n",
      "Step 76: Loss: -0.998028576374054\n",
      "Step 77: Loss: -0.9985038638114929\n",
      "Step 78: Loss: -0.9987630248069763\n",
      "Step 79: Loss: -0.998889684677124\n",
      "Step 80: Loss: -0.9989679455757141\n",
      "Step 81: Loss: -0.9990224838256836\n",
      "Step 82: Loss: -0.9990944266319275\n",
      "Step 83: Loss: -0.9992824792861938\n",
      "Step 84: Loss: -0.9995669722557068\n",
      "Step 85: Loss: -0.9998330473899841\n",
      "Step 86: Loss: -0.9999532103538513\n",
      "Step 87: Loss: -0.9999043345451355\n",
      "Step 88: Loss: -0.9997946619987488\n",
      "Step 89: Loss: -0.9997189044952393\n",
      "Step 90: Loss: -0.9996761679649353\n",
      "Step 91: Loss: -0.9996726512908936\n",
      "Step 92: Loss: -0.9997386336326599\n",
      "Step 93: Loss: -0.9998448491096497\n",
      "Step 94: Loss: -0.9999212622642517\n",
      "Step 95: Loss: -0.9999069571495056\n",
      "Step 96: Loss: -0.9998587369918823\n",
      "Step 97: Loss: -0.9998379945755005\n",
      "Step 98: Loss: -0.9998499155044556\n",
      "Step 99: Loss: -0.9998738765716553\n",
      "Step 100: Loss: -0.9999143481254578\n"
     ]
    }
   ],
   "source": [
    "steps = 100\n",
    "\n",
    "weights = torch.rand(n_qubits, 2, requires_grad=True)\n",
    "opt = torch.optim.AdamW([weights], lr=0.1)\n",
    "\n",
    "for i in range(steps):\n",
    "    opt.zero_grad()\n",
    "    loss = cost_function(weights)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"Step {}: Loss: {}\".format(i + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU0:  1.0000444527595604\n"
     ]
    }
   ],
   "source": [
    "indices = [marg for var in head for marg in variables_to_marginals[var]]\n",
    "hu0 = circuit(weights, indices)\n",
    "total = entropy(hu0).item()\n",
    "print(\"HU0: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd tensor(1.3193e-05, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "fd tensor(4.1908e-05, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "fd tensor(1.8263e-06, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "fd tensor(6.7317e-06, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "fd tensor(1.3846e-05, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "fd tensor(2.1477e-06, dtype=torch.float64, grad_fn=<SubBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "rel  tensor(1.0000, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for fd in fds:\n",
    "    indices1 = [marg for var in fd[0] + fd[1] for marg in variables_to_marginals[var]]\n",
    "    indices2 = [marg for var in fd[1] for marg in variables_to_marginals[var]]\n",
    "    marginal1 = circuit(weights, indices1)\n",
    "    marginal2 = circuit(weights, indices2)\n",
    "    entr1 = entropy(marginal1)\n",
    "    entr2 = entropy(marginal2)\n",
    "    print(\"fd\", entr1 - entr2)\n",
    "    \n",
    "for relation in relations:\n",
    "    indices = [marg for var in relation for marg in variables_to_marginals[var]]\n",
    "    marginal = circuit(weights, indices)\n",
    "    entr = entropy(marginal)\n",
    "    print(\"rel \", entr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.0887e-04,  5.1333e-01],\n",
      "        [-1.5722e+00,  4.2235e-02],\n",
      "        [ 1.2741e-03, -1.0305e-02],\n",
      "        [-2.0693e-03, -3.0259e-02],\n",
      "        [ 1.3217e-03, -2.6881e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
